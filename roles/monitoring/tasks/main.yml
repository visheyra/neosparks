- name: Setup CAdvisor for containers monitoring
  docker_container:
    name: cadvisor
    image: "{{ downloads.cadvisor.image }}:{{ downloads.cadvisor.tag }}"
    state: "{{ allspark_monitoring.enabled and 'started' or 'absent' }}"
    volumes: >-
      [
        {%- if (ansible_distribution == 'CentOS' or ansible_distribution == 'RedHat') -%}
        {# CentOS/RHEL specific mount point : https://github.com/google/cadvisor/blob/master/docs/running.md#centos-fedora-and-rhel #}
        "/cgroup:/cgroup:ro",
        {%- endif -%}
        "/sys:/sys:ro",
        "/var/lib/docker/:/var/lib/docker:ro",
        "/var/run:/var/run:rw",
        "/:/rootfs:ro"
      ]
    purge_networks: true
    privileged: "{%- if not (ansible_distribution == 'CentOS' or ansible_distribution == 'RedHat') -%}false{%- else -%}true{%- endif -%}"
    networks:
      - name: allspark
    restart_policy: always
    labels:
      "heritage": "allspark"

- name: HAProxy exporter
  docker_container:
    name: haproxy_exporter
    image: "{{ downloads.haproxy_exporter.image }}:{{ downloads.haproxy_exporter.tag }}"
    state: "{{ allspark_monitoring.enabled and 'started' or 'absent' }}"
    purge_networks: true
    user: "1000:1000"
    command: --haproxy.scrape-uri="http://haproxy:9000/metrics?stats;csv"
    networks:
      - name: allspark
    restart_policy: always
    labels:
      "heritage": "allspark"

- name: Setup node exporter
  docker_container:
    name: node-exporter
    image: "{{ downloads.node_exporter.image }}:{{ downloads.node_exporter.tag }}"
    state: "{{ allspark_monitoring.enabled and 'started' or 'absent' }}"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - --collector.filesystem.ignored-mount-points
      - "^/(sys|proc|dev|host|etc|rootfs/var/lib/docker/containers|rootfs/var/lib/docker/overlay2|rootfs/run/docker/netns|rootfs/var/lib/docker/aufs|rootfs/var/lib/docker/devicemapper|rootfs/var/lib/docker/plugins|rootfs/run/user)($$|/)"
    purge_networks: true
    networks:
      - name: allspark
    restart_policy: always
    labels:
      "heritage": "allspark"

- name: Create Prometheus config folder
  file:
    state: directory
    path: "{{ allspark_root_directory }}/config/prometheus"
    owner: "{{ ansible_user }}"
    group: "{{ ansible_user }}"
  when: allspark_monitoring.enabled

- name: Generate Prometheus configuration
  template:
    src: templates/prometheus.yml.j2
    dest: "{{ allspark_root_directory }}/config/prometheus/prometheus.yml"
  when: allspark_monitoring.enabled

- name: Create Prometheus volume
  docker_volume:
    name: "allspark_prometheus_data"
  when: allspark_monitoring.enabled

- name: Setup Prometheus metrics collector
  docker_container:
    name: prometheus
    image: "{{ downloads.prometheus.image }}:{{ downloads.prometheus.tag }}"
    state: "{{ allspark_monitoring.enabled and 'started' or 'absent' }}"
    volumes:
      - "{{ allspark_root_directory }}/config/prometheus/:/etc/prometheus/"
      - allspark_prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    purge_networks: true
    networks:
      - name: allspark
    restart_policy: always
    labels:
      "heritage": "allspark"

- name: Generate Alertmanager configuration
  template:
    src: templates/alertmanager.yml.j2
    dest: "{{ allspark_root_directory }}/config/prometheus/alertmanager.yml"
  when: allspark_monitoring.enabled

- name: Create Alertmanager volume
  docker_volume:
    name: "allspark_alertmanager_data"
  when: allspark_monitoring.enabled

- name: Setup Prometheus metrics collector
  docker_container:
    name: prometheus_alertmanager
    image: "{{ downloads.prometheus_alertmanager.image }}:{{ downloads.prometheus_alertmanager.tag }}"
    state: "{{ allspark_monitoring.enabled and 'started' or 'absent' }}"
    volumes:
      - "{{ allspark_root_directory }}/config/prometheus/alertmanager.yml:/etc/alertmanager/alertmanager.yml"
      - allspark_alertmanager_data:/alertmanager
    purge_networks: true
    networks:
      - name: allspark
    restart_policy: always
    labels:
      "heritage": "allspark"

# TODO: Skip this when grafana is disabled
# Not possible to just skip the task as some files are needed for the container
# setup task (even with state: absent)
- name: Send grafana config
  copy:
    src: files/grafana
    dest: "{{ allspark_root_directory }}/config"

- name: Ensuring grafana init script is executable
  file: dest="{{ allspark_root_directory }}/config/grafana/init.sh" mode=a+x
  when: allspark_monitoring.enabled

- name: Setup Grafana for vizualization
  docker_container:
    name: grafana
    image: "{{ downloads.grafana.image }}:{{ downloads.grafana.tag }}"
    state: "{{ allspark_monitoring.enabled and 'started' or 'absent' }}"
    volumes:
      - allspark_grafana_data:/var/lib/grafana
      - "{{ allspark_root_directory }}/config/grafana/provisioning/:/etc/grafana/provisioning/"
      - "{{ allspark_root_directory }}/data/secrets/admin_password.txt:/run/secrets/grafana_admin_password.txt"
      - "{{ allspark_root_directory }}/config/grafana/init.sh:/etc/grafana/allspark_init.sh"
    env_file: "{{ allspark_root_directory }}/config/grafana/config.monitoring"
    entrypoint: /etc/grafana/allspark_init.sh
    restart_policy: always
    purge_networks: true
    networks:
      - name: allspark
    labels:
      "heritage": "allspark"
